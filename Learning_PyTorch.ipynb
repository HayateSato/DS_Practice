{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPD76tPjgPMhOvDva6j3sA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayateSato/DS_Practice/blob/main/Learning_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IzifK7qnfmq",
        "outputId": "bc2c404a-eaf0-4964-8846-77bde96e2de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "cpu\n",
            "torch.int64\n"
          ]
        }
      ],
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "list_a = [1, 2, 3, 4]\n",
        "\n",
        "# Create a tensor from list_a\n",
        "tensor_a = torch.tensor(list_a)\n",
        "list_a\n",
        "\n",
        "print(tensor_a)\n",
        "\n",
        "# Display the tensor device\n",
        "print(tensor_a.device)\n",
        "\n",
        "# Display the tensor data type\n",
        "print(tensor_a.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor can be calculated like numpy array"
      ],
      "metadata": {
        "id": "rTJvgtRzpgfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_a = [1, 2, 3, 4]\n",
        "array_b = [5, 6, 7, 8]\n",
        "\n",
        "# Create two tensors from the arrays\n",
        "tensor_a = torch.tensor(array_a)\n",
        "tensor_b = torch.tensor(array_b)\n",
        "print(f\"a: {tensor_a}\")\n",
        "print(f\"b: {tensor_b}\")\n",
        "\n",
        "# Subtract tensor_b from tensor_a\n",
        "tensor_c = tensor_a - tensor_b\n",
        "print(f\"c: {tensor_c}\")\n",
        "\n",
        "# Multiply each element of tensor_a with each element of tensor_b\n",
        "tensor_d = tensor_a * tensor_b\n",
        "print(f\"d: {tensor_d}\")\n",
        "\n",
        "# Add tensor_c to tensor_d\n",
        "tensor_e = tensor_c + tensor_d\n",
        "print(f\"e: {tensor_e}\")\n",
        "\n",
        "# concatenating\n",
        "tensor_f = torch.cat((tensor_a, tensor_b))\n",
        "print(f\"f: {tensor_f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVlGSWpxoLWG",
        "outputId": "7dfbd570-677a-4c90-f786-94453a574812"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([1, 2, 3, 4])\n",
            "b: tensor([5, 6, 7, 8])\n",
            "c: tensor([-4, -4, -4, -4])\n",
            "d: tensor([ 5, 12, 21, 32])\n",
            "e: tensor([ 1,  8, 17, 28])\n",
            "f: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Nueral Netowork"
      ],
      "metadata": {
        "id": "TZkyv0ImpsEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])  ### if you want to defin a tensor direclty without using a variable, you need double [[ ]]\n",
        "\n",
        "# Implement a small neural network with exactly two linear layers\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 1),\n",
        "    nn.Linear(1, 1)\n",
        "                     )\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ueCI-rdpvKZ",
        "outputId": "0caedb77-78f3-4da2-dbda-fe6b73f64c73"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3729]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "at the end of the nn, you need to activate it by calling a function such as\n",
        "- Sigmoid\n",
        "- Softmax\n",
        "- ReLU\n",
        "- LU\n",
        "etc.\n",
        "\n",
        "**Sigmoid is used for binary classification methods where we only have 2 classes, while SoftMax applies to multiclass problem**"
      ],
      "metadata": {
        "id": "UtBhFns7yDiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sigmoid function"
      ],
      "metadata": {
        "id": "LKPfY7BNqX5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor([[0.8]])\n",
        "\n",
        "# Create a sigmoid function and apply it on input_tensor\n",
        "sigmoid = nn.Sigmoid()\n",
        "probability = sigmoid(input_tensor)\n",
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAFL4lRwqbDg",
        "outputId": "5c464835-6de0-4a31-f781-e0e16d462e8d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6900]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Implement a neural network with exactly four linear layers\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(11, 4),\n",
        "    nn.Linear(4,3),\n",
        "    nn.Linear(3,2),\n",
        "    nn.Linear(2,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXqsgwC-xH7P",
        "outputId": "b458ca22-1b0d-445e-bcf2-252216e5b0a5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4793]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### softmax function"
      ],
      "metadata": {
        "id": "rXuipbRmve9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
        "\n",
        "# Create a softmax function and apply it on input_tensor\n",
        "softmax = nn.Softmax()\n",
        "probabilities = softmax(input_tensor)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FnBs927vdoB",
        "outputId": "dcf76fd9-5e68-4894-fdab-c00b9591fd7a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Update network below to perform a multi-class classification with four labels\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(11, 20),\n",
        "  nn.Linear(20, 12),\n",
        "  nn.Linear(12, 6),\n",
        "  nn.Linear(6, 4),\n",
        "  nn.Softmax()\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGp4OwKaxNV2",
        "outputId": "7f82dd4a-36de-42d0-dd1f-bf3646d1f217"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0804, 0.1197, 0.0484, 0.7515]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hot Encoding\n",
        "One-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset\n",
        "\n",
        " how to this encoding compare with / without pytorch.\n",
        "    1. first without\n",
        "    2. then with pytorch"
      ],
      "metadata": {
        "id": "PAv4mULtyoPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# without using pytorch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "y = 1\n",
        "num_classes = 3\n",
        "\n",
        "# Create the one-hot encoded vector using NumPy\n",
        "one_hot_numpy = np.array([0, 1, 0])\n",
        "\n",
        "# Create the one-hot encoded vector using PyTorch\n",
        "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes)\n",
        "\n",
        "print(one_hot_numpy)\n",
        "print(one_hot_pytorch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoknizmWw-Tt",
        "outputId": "3188e281-390b-4bcc-e08f-b9c3ed892d61"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0]\n",
            "tensor([0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "y = [2]\n",
        "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
        "num_scores_tensors = scores.shape[1]\n",
        "\n",
        "# Create a one-hot encoded vector of the label y\n",
        "one_hot_label = F.one_hot(torch.tensor(y), num_classes = num_scores_tensors)  ### converting the result of y into one-hot encoded vector to save it\n",
        "\n",
        "# Create the cross entropy loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Calculate the cross entropy loss\n",
        "loss = criterion(scores.double(), one_hot_label.double())\n",
        "print(loss)\n",
        "\n",
        "\n",
        "print(scores)\n",
        "print(one_hot_label)\n",
        "print(criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MikIK8s43X8T",
        "outputId": "b9552ef7-df95-45b3-e14f-fbd92d88c8a7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.0619, dtype=torch.float64)\n",
            "tensor([[ 0.1000,  6.0000, -2.0000,  3.2000]])\n",
            "tensor([[0, 0, 1, 0]])\n",
            "CrossEntropyLoss()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "weight =  torch.randn(2, 9, requires_grad=True)\n",
        "bias = torch.randn(2, requires_grad=True)\n",
        "preds = torch.randn(1, 2, requires_grad=True)\n",
        "target = torch.tensor([[1.0, 0.0]], requires_grad=False)\n",
        "\n",
        "# Calculate the loss\n",
        "loss = criterion(preds, target)\n",
        "\n",
        "# Compute the gradients of the loss\n",
        "loss.backward()\n",
        "\n",
        "# Display gradients of the weight and bias tensors in order\n",
        "print(weight.grad)\n",
        "print(bias.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o1tK8kqkUF3",
        "outputId": "3cbdf6fc-2471-41c5-b0a8-b0f36993c5df"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8IVOwkfClv1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### each layer parameter can be accessed by indexing the created model directly\n",
        "\n",
        "model = nn.Sequential(nn.Linear(16, 8),\n",
        "                      nn.Sigmoid(),\n",
        "                      nn.Linear(8, 2))\n",
        "\n",
        "# Access the weight of the first linear layer\n",
        "weight_0 =  model[0].weight\n",
        "\n",
        "# Access the bias of the second linear layer\n",
        "bias_1 = model[2].bias\n",
        "\n",
        "print(model)\n",
        "# Sequential(\n",
        "#   (0): Linear(in_features=16, out_features=8, bias=True)\n",
        "#   (1): Sigmoid()\n",
        "#   (2): Linear(in_features=8, out_features=2, bias=True)\n",
        "# )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4A7RcMklkTR",
        "outputId": "42fc3d36-5c03-47e9-ed5c-8d921a778290"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=8, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "loss = criterion(preds, target)\n",
        "loss.backward()\n",
        "\n",
        "# Update the model's parameters using the optimizer\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "K5oELSjWmNgO"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEzmypvPml60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   "
      ],
      "metadata": {
        "id": "JBQCM_0zzq9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can't use cross-entropy loss for regression problems. The mean squared error loss (MSELoss) is a common loss function for regression problems."
      ],
      "metadata": {
        "id": "TRSdeEH64EdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = np.array(10)\n",
        "y = np.array(1)\n",
        "\n",
        "# Calculate the MSELoss using NumPy\n",
        "mse_numpy = np.mean((y_hat - y)**2)\n",
        "\n",
        "# Create the MSELoss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Calculate the MSELoss using the created loss function\n",
        "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
        "print(mse_pytorch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS9YsFbjnSxJ",
        "outputId": "a549f29b-6ee2-405e-a124-9276ab5a4b39"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(81.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In scikit-learn, the whole training loop is contained in the .fit() method. In PyTorch, however, you implement the loop manually. While this provides control over loop's content, it requires a custom implementation."
      ],
      "metadata": {
        "id": "LsfMw_4QnhqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import dataloader, TensorDataset\n",
        "\n",
        "# # Loop over the number of epochs and then the dataloader\n",
        "# for epoch in range(num_epochs):\n",
        "#   for data in dataloader:\n",
        "#     # Set the gradients to zero\n",
        "#     optimizer.zero_grad()\n",
        ""
      ],
      "metadata": {
        "id": "uVpixZcHnlEr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU is one of the most used activation functions in deep learning.  "
      ],
      "metadata": {
        "id": "nhLqei0RoKDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ReLU function with PyTorch\n",
        "relu_pytorch = nn.ReLU()\n",
        "\n",
        "# Apply your ReLU function on x, and calculate gradients\n",
        "x = torch.tensor(-1.0, requires_grad=True)\n",
        "y = relu_pytorch(x)\n",
        "y.backward()\n",
        "\n",
        "# Print the gradient of the ReLU function for x\n",
        "gradient = x.grad\n",
        "print(gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkbezzWDoFqc",
        "outputId": "ef727ec6-896b-4ebc-af32-3ae449836b1f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, inconvenience is that ReLU outputs null values for negative inputs and therefore, having null gradients. Once an element of the input is negative, it will be set to zero for the rest of the training.\n",
        "\n",
        "Leaky ReLU overcomes this challenge by using a multiplying factor for negative inputs."
      ],
      "metadata": {
        "id": "myP20rnUoiDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a leaky relu function in PyTorch\n",
        "leaky_relu_pytorch = nn.LeakyReLU(negative_slope=0.05)\n",
        "\n",
        "x = torch.tensor(-2.0)\n",
        "# Call the above function on the tensor x\n",
        "output = leaky_relu_pytorch(x)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKbA-o0woz09",
        "outputId": "93312a3b-6233-41eb-a1a0-f3ffa030c65a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting the number of parameters   .numel()"
      ],
      "metadata": {
        "id": "0OdWcUTxpQQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(16, 4),\n",
        "                      nn.Linear(4, 2),\n",
        "                      nn.Linear(2, 1))\n",
        "\n",
        "total = 0\n",
        "\n",
        "# Calculate the number of parameters in the model\n",
        "for parameter in model.parameters():\n",
        "  total += parameter.numel()\n",
        "\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_B2ydQ_pM2d",
        "outputId": "e83c5423-a0b1-40b1-e655-819ce62a266c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_capacity(model):\n",
        "  total = 0\n",
        "  for p in model.parameters():\n",
        "    total += p.numel()\n",
        "  return total\n",
        "\n",
        "n_features = 8\n",
        "n_classes = 2\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Create a neural network with less than 120 parameters\n",
        "model = nn.Sequential(nn.Linear(n_features, 3),\n",
        "                      nn.Linear(3, 2),\n",
        "                      nn.Linear(2, n_classes)\n",
        "\n",
        ")\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(calculate_capacity(model))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIywXseypM9k",
        "outputId": "fb71d042-49b4-457d-b2f1-ddf73d818727"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMl-Bp5tpNCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing layers of a model\n",
        "\n",
        "when fin-tuning the model and we want to modify only layer of the model, you can freeze the irrelevant layers. (param.requires_grad = False)"
      ],
      "metadata": {
        "id": "J71-3kF2qQyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "\n",
        "    # Check if the parameters belong to the first layer\n",
        "    if name == '0.weight' or name == '0.bias':\n",
        "\n",
        "        # Freeze the parameters\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Check if the parameters belong to the second layer\n",
        "    if name == '1.weight' or name == '1.bias':\n",
        "\n",
        "        # Freeze the parameters\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "pRwifFwpqWDw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight initilization\n",
        "\n",
        "before training the model, it is recommneded to adjust the weight"
      ],
      "metadata": {
        "id": "DpFDuztSr-HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example of a simple neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the network\n",
        "net = SimpleNet()\n",
        "\n",
        "# Apply Xavier initialization for weights of fc1 and fc2 layers\n",
        "nn.init.xavier_uniform_(net.fc1.weight)\n",
        "nn.init.xavier_uniform_(net.fc2.weight)\n",
        "\n",
        "# You can also apply He initialization like this\n",
        "# nn.init.kaiming_uniform_(net.fc1.weight, nonlinearity='relu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1_N-cjzr9h8",
        "outputId": "2e254c45-caa1-40bc-9966-ed25a3ba85c9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0037,  0.1932, -0.1306, -0.1087, -0.2028, -0.1138, -0.0124,  0.2598,\n",
              "          0.0008, -0.2163,  0.2444,  0.1109, -0.0601, -0.0899,  0.0342,  0.0859,\n",
              "         -0.2702,  0.2065,  0.0647, -0.0398, -0.0287, -0.1281, -0.1197,  0.0672,\n",
              "         -0.2232,  0.1178,  0.0742,  0.1617, -0.1814, -0.2682,  0.1216,  0.2613,\n",
              "          0.1424, -0.2515,  0.0200,  0.0511, -0.2564,  0.2331,  0.2840,  0.0266,\n",
              "          0.0019,  0.2093, -0.1224,  0.0209,  0.2475, -0.1958, -0.1507, -0.1261,\n",
              "          0.0410,  0.2797, -0.2322, -0.2240,  0.2213,  0.1751, -0.2284,  0.0255,\n",
              "          0.0418,  0.2730, -0.2570,  0.0528,  0.2512,  0.1491,  0.0326,  0.0453],\n",
              "        [ 0.2195, -0.1254,  0.1707, -0.1754, -0.1837,  0.1502, -0.0964,  0.0603,\n",
              "          0.1129, -0.2163, -0.2155, -0.1992,  0.1918, -0.0478,  0.1038, -0.0802,\n",
              "         -0.1880, -0.2243,  0.1718,  0.2202,  0.2425, -0.2412, -0.0345, -0.2508,\n",
              "          0.1646,  0.0050, -0.0337,  0.0720, -0.0190, -0.0603,  0.2253, -0.1521,\n",
              "          0.0167,  0.0243, -0.1194, -0.0100, -0.0973,  0.2212,  0.2653,  0.0332,\n",
              "         -0.1265,  0.1929, -0.0038,  0.2513, -0.1049,  0.1027,  0.0434, -0.0338,\n",
              "          0.2814, -0.1545, -0.1704, -0.0524, -0.1433, -0.1872,  0.2607,  0.1038,\n",
              "         -0.1367, -0.0931, -0.1854, -0.2065, -0.1430, -0.1167, -0.2763, -0.0112],\n",
              "        [-0.0283,  0.1231, -0.2538, -0.1737,  0.0787, -0.0331, -0.0094, -0.0910,\n",
              "          0.2498,  0.0242, -0.2058, -0.0572,  0.1029, -0.0534,  0.1577,  0.2472,\n",
              "          0.0802, -0.0119, -0.2051, -0.0932, -0.0131,  0.1388,  0.1388,  0.0734,\n",
              "          0.2511,  0.1064,  0.0037, -0.2349,  0.2180,  0.2508,  0.1225, -0.2191,\n",
              "         -0.0839, -0.1922, -0.1975,  0.2664, -0.0284,  0.1891,  0.2680, -0.0033,\n",
              "          0.0420,  0.1464,  0.1743,  0.2754,  0.1412,  0.2810,  0.0005, -0.0546,\n",
              "         -0.0510,  0.1669,  0.0568,  0.1844,  0.1538, -0.0143, -0.1204,  0.2051,\n",
              "          0.2541,  0.2722,  0.2517, -0.0687, -0.1303,  0.1055,  0.0094,  0.1484],\n",
              "        [ 0.1571, -0.2146,  0.0655, -0.1943,  0.2499,  0.0203, -0.0909,  0.1203,\n",
              "         -0.1495,  0.1009,  0.1627, -0.1748,  0.1182, -0.0227,  0.2749,  0.0365,\n",
              "          0.0782, -0.1734, -0.2213,  0.0676,  0.0739, -0.0704, -0.1613,  0.0747,\n",
              "          0.0766, -0.0573,  0.2532,  0.0291, -0.2443,  0.2141, -0.0088,  0.0864,\n",
              "         -0.1858,  0.2513,  0.2822,  0.2430, -0.2218, -0.1616, -0.0179,  0.1332,\n",
              "          0.1553,  0.1698, -0.0612, -0.0586, -0.0987,  0.1775,  0.2258, -0.2747,\n",
              "          0.2737,  0.0510, -0.0572, -0.1682, -0.1830,  0.2166,  0.2445, -0.1899,\n",
              "          0.0217, -0.0479,  0.2662, -0.2585, -0.2548, -0.0171, -0.2181, -0.0496],\n",
              "        [-0.2691, -0.2479, -0.1821,  0.0787,  0.0743,  0.2776, -0.0611,  0.1626,\n",
              "          0.1121, -0.2562, -0.0539,  0.2620, -0.2200, -0.1401,  0.2133, -0.1902,\n",
              "          0.1827,  0.0153, -0.2803, -0.0765, -0.0811, -0.1189,  0.1899,  0.1802,\n",
              "          0.2421,  0.1603, -0.0231,  0.0738, -0.1070, -0.1866, -0.2450,  0.0299,\n",
              "          0.1376, -0.2625,  0.1605, -0.1935, -0.1861,  0.0017,  0.2585,  0.1541,\n",
              "         -0.2579, -0.0020, -0.1253, -0.2035, -0.1058, -0.1318, -0.2303, -0.0669,\n",
              "          0.0188,  0.1316,  0.2574,  0.2030, -0.0900, -0.0970,  0.1161,  0.0811,\n",
              "          0.1547,  0.2822, -0.2031, -0.2595,  0.0251,  0.2841, -0.1978,  0.0772],\n",
              "        [-0.0039, -0.1665, -0.1409, -0.0728,  0.2203,  0.1574, -0.2239,  0.2171,\n",
              "         -0.2068, -0.1950, -0.1332, -0.0256, -0.0434,  0.0500, -0.2570, -0.2024,\n",
              "         -0.2048, -0.2651,  0.0336,  0.2787, -0.0161,  0.0825, -0.0782,  0.2838,\n",
              "          0.1026,  0.0139, -0.0266, -0.1669, -0.1697, -0.0366,  0.0089,  0.0267,\n",
              "          0.1969, -0.1237,  0.1530, -0.2633,  0.1035, -0.0377, -0.1912, -0.2179,\n",
              "         -0.0870, -0.0141,  0.2844,  0.1444,  0.0502, -0.0091, -0.1886, -0.2475,\n",
              "          0.2012,  0.2722,  0.2345,  0.2281,  0.2584, -0.1758, -0.1925,  0.2676,\n",
              "         -0.0087,  0.1700,  0.2491, -0.0447, -0.2353, -0.2537,  0.0442,  0.0184],\n",
              "        [-0.1610,  0.2259, -0.1812, -0.2336,  0.2464,  0.2146, -0.2628,  0.0053,\n",
              "         -0.1399,  0.1816, -0.0770,  0.1354,  0.2319,  0.1883,  0.0266,  0.0854,\n",
              "          0.1990, -0.0676,  0.1917,  0.2436, -0.2410,  0.2080, -0.0833, -0.1085,\n",
              "          0.2517, -0.0805, -0.1702, -0.1352, -0.1894,  0.0409, -0.1062, -0.2015,\n",
              "          0.0066, -0.1981,  0.2474, -0.0281,  0.1515,  0.1181, -0.2646,  0.2259,\n",
              "         -0.2422,  0.0563, -0.2335, -0.0795,  0.1756,  0.2061, -0.0831,  0.1781,\n",
              "          0.2064,  0.1263,  0.2617,  0.0651, -0.2079, -0.0868, -0.2626, -0.2537,\n",
              "          0.2391,  0.2252, -0.0386, -0.1766, -0.0928, -0.2804,  0.2174, -0.0940],\n",
              "        [-0.0675, -0.1047,  0.2095, -0.2784,  0.2125,  0.0816, -0.0613,  0.2089,\n",
              "          0.0039, -0.1493,  0.2626,  0.1087, -0.2516, -0.0322,  0.0758, -0.1720,\n",
              "         -0.1796,  0.1251,  0.1191,  0.1442,  0.2272, -0.2756,  0.2689, -0.0701,\n",
              "         -0.2686, -0.0185,  0.1726, -0.2399,  0.1460,  0.2523,  0.1591, -0.1205,\n",
              "          0.1116,  0.1248,  0.0547, -0.0015,  0.1250,  0.1834,  0.0427,  0.1857,\n",
              "         -0.1199, -0.0048,  0.2058, -0.0711,  0.1615,  0.2334, -0.1121,  0.0701,\n",
              "         -0.0256, -0.1663, -0.1814,  0.1406,  0.2053,  0.1629, -0.1835,  0.0131,\n",
              "          0.2192,  0.2062,  0.0074, -0.1960, -0.1633, -0.0201, -0.2212,  0.0792],\n",
              "        [ 0.1519, -0.0588, -0.0497, -0.1177, -0.2250,  0.0203, -0.1774, -0.0306,\n",
              "         -0.0224, -0.2833, -0.1391,  0.0646,  0.2633,  0.2158,  0.1514, -0.0986,\n",
              "          0.0375, -0.0532,  0.2230,  0.2265,  0.1229, -0.0869,  0.1503, -0.0197,\n",
              "         -0.0999,  0.2453, -0.0833, -0.0893,  0.1412,  0.2489, -0.0400, -0.0200,\n",
              "         -0.1890, -0.1230,  0.0509, -0.2390,  0.0712, -0.2666, -0.2424, -0.1201,\n",
              "          0.1922,  0.2819,  0.2770,  0.2667, -0.2708,  0.0438,  0.1634, -0.2293,\n",
              "         -0.2288,  0.0979,  0.0461,  0.0577, -0.2791,  0.1435, -0.2288, -0.2586,\n",
              "          0.0934, -0.0604, -0.0686, -0.0102,  0.1418,  0.1339, -0.1913, -0.1496],\n",
              "        [ 0.2791, -0.1013,  0.2245, -0.1774, -0.1489,  0.1354,  0.0742, -0.2170,\n",
              "          0.1024,  0.1644, -0.1976, -0.2502,  0.1918,  0.2400, -0.2010,  0.0305,\n",
              "         -0.0394,  0.1782,  0.1801,  0.1199,  0.1736, -0.0004,  0.2189,  0.0987,\n",
              "         -0.2063, -0.1562,  0.2796, -0.0331,  0.2463,  0.1852, -0.0198,  0.1905,\n",
              "          0.0470, -0.1804, -0.1555, -0.0429, -0.0659, -0.1842, -0.0447,  0.0604,\n",
              "         -0.2711, -0.2549,  0.0892, -0.0505, -0.1866, -0.2509, -0.0137, -0.1310,\n",
              "          0.2077,  0.0980,  0.1294,  0.1067,  0.1888, -0.1822,  0.1477, -0.1840,\n",
              "         -0.1211,  0.0799, -0.0593,  0.0627, -0.1215,  0.1350,  0.0212,  0.1340]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6UuFMqd6sFq3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}